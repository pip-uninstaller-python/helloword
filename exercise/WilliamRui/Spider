import requests
from bs4 import BeautifulSoup

headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36'}
#请求头，用于requests中模拟浏览器操作，看需要可以加上Cookie和Referer等

def GetSoup(url):
#程序中多处用到BeautifulSoup，定义此函数减少代码量
#如有网址涉及到在请求头中加上Referer，在使用这个函数之前要对请求头进行初始化操作。
    Resp = requests.get(url,headers=headers)
    #获取网址页面
    Resp.encoding = Resp.apparent_encoding
    #某些情况下，网页的编码方式不是utf-8，此处统一编码方式？
    Soup = BeautifulSoup(Resp.text,'html.parser')
    #转换为Soup
    return Soup

def GetBookUrl():
#此函数用于根据使用者输入，生成网站索引后界面的Url
#根据观察，索引后网址为xxxxxxxxxxxxxxx + 小说名（浏览器上实际显示是小说名转换来的特殊字符）
    while True:
        BookName = input('请输入要下载的小说名：')
        #将小说名存储在变量BookName中
        OriginalUrl = 'https://www.xbiquge6.com/search.php?keyword=' + BookName
        #拼接网址和BookName，产生实际的网址
        Soup = GetSoup(OriginalUrl)
        Books = Soup.find('div',class_='result-list').findAll('div',class_='result-item result-game-item')
        print('正在查找小说...')
        for Book in Books:
            BookName1 = Book.find('div',class_='result-game-item-detail').find('h3').find('a').get('title')
            BookLink1 = Book.find('div',class_='result-game-item-detail').find('h3').find('a').get('href')
            if BookName == BookName1:
            #因索引结果中有很多只是名字包含搜索的小说名，这里做一个判断，必须要和搜索的小说名完全一致才可以
                return BookLink1,BookName
                #输出地址和小说名，小说名在后续下载后的文件命名中使用
        else:
            print('未发现此书')
            continue
            #如果没有找到书，则重新输入

def GetChapterLinks(url):
    Soup = GetSoup(url)
    ChapterNames = []
    #定义一个列表存放章节名称，后续会放在下载下来的章节之前
    ChapterLinks = []
    #定义一个列表存放章节地址，此处使用的是一次性获取，可以考虑在每一个章节的下一章链接上获取地址。
    Chapters = Soup.find('div',id='list').find('dl').findAll('dd')
    for Chapter in Chapters:
        ChapterNames.append(Chapter.find('a').get_text())
        ChapterLinks.append('https://www.xbiquge6.com'+ Chapter.find('a').get('href'))
        #通过观察，获取到的地址只有后面半部分，前面部分手动添加进去
    return ChapterNames,ChapterLinks
 
def GetContent(url):
#获取章节的正文
    Soup = GetSoup(url)
    Content = Soup.find('div',id='content').text
    #获取章节正文
    Content = Content.replace('\u00a0\u00a0\u00a0\u00a0','\n')
    #网站上的小说内容的换行是用<br>实现的，我们获取的内容不会获取到这个，所以爬下来的文本中没有换行。
    #通过观察，每一段前面的缩进是有识别的，所以我们这里将缩进替换为换行加缩进，这样爬下来的内容就有换行了。
    return Content
    
BookUrl,BookName = GetBookUrl()
ChanpterNames,ChanpterLinks = GetChapterLinks(BookUrl)
with open(BookName + '.txt','a',encoding='utf-8') as T:
#正文中的&nbsp字符（前面提到的缩进，即空格）无法被encode，所以这里我们用utf-8的编码方式
    print('正在下载中.......')
    for i in range(len(ChanpterNames)):
        T.write(ChanpterNames[i])
        T.write('\n')
        #在章节名和章节正文之间空一行，方便阅读
        T.write(GetContent(ChanpterLinks[i]))
        T.write('\n')
        #在章节正文和章节名之间空一行，方便阅读
    print('下载完成')
        

    
